
# DeepWiki-Open

![BanniÃ¨re DeepWiki](screenshots/Deepwiki.png)

**DeepWiki** est ma propre tentative dâ€™implÃ©mentation de DeepWiki, un outil qui crÃ©e automatiquement des wikis magnifiques et interactifs pour nâ€™importe quel dÃ©pÃ´t GitHub, GitLab ou Bitbucket ! Il suffit dâ€™entrer un nom de dÃ©pÃ´t, et DeepWiki :

1. Analyse la structure du code  
2. GÃ©nÃ¨re une documentation complÃ¨te  
3. CrÃ©e des diagrammes visuels pour expliquer le fonctionnement  
4. Organise le tout dans un wiki facile Ã  naviguer

[!["Buy Me A Coffee"](https://www.buymeacoffee.com/assets/img/custom_images/orange_img.png)](https://buymeacoffee.com/sheing)
[![Tip in Crypto](https://tip.md/badge.svg)](https://tip.md/sng-asyncfunc)
[![Twitter/X](https://img.shields.io/badge/Twitter-1DA1F2?style=for-the-badge&logo=twitter&logoColor=white)](https://x.com/sashimikun_void)
[![Discord](https://img.shields.io/badge/Discord-7289DA?style=for-the-badge&logo=discord&logoColor=white)](https://discord.com/invite/VQMBGR8u5v)

[English](./README.md) | [ç®€ä½“ä¸­æ–‡](./README.zh.md) | [ç¹é«”ä¸­æ–‡](./README.zh-tw.md) | [æ—¥æœ¬èª](./README.ja.md) | [EspaÃ±ol](./README.es.md) | [í•œêµ­ì–´](./README.kr.md) | [Tiáº¿ng Viá»‡t](./README.vi.md) | [PortuguÃªs Brasileiro](./README.pt-br.md) | [FranÃ§ais](./README.fr.md)

## âœ¨ FonctionnalitÃ©s

- **Documentation instantanÃ©e** : Transforme un dÃ©pÃ´t GitHub, GitLab ou Bitbucket en wiki en quelques secondes
- **Support des dÃ©pÃ´ts privÃ©s** : AccÃ¨s sÃ©curisÃ© avec jetons dâ€™accÃ¨s personnels
- **Analyse intelligente** : ComprÃ©hension de la structure et des relations du code via lâ€™IA
- **Diagrammes Ã©lÃ©gants** : Diagrammes Mermaid automatiques pour visualiser lâ€™architecture et les flux de donnÃ©es
- **Navigation facile** : Interface simple et intuitive
- **Fonction â€œAskâ€** : Posez des questions Ã  votre dÃ©pÃ´t avec une IA alimentÃ©e par RAG
- **DeepResearch** : Processus de recherche multi-Ã©tapes pour explorer des sujets complexes
- **Multiples fournisseurs de modÃ¨les IA** : Prise en charge de Google Gemini, OpenAI, OpenRouter, et Ollama local

## ğŸš€ DÃ©marrage rapide (super facile !)

### Option 1 : Avec Docker

```bash
# Cloner le dÃ©pÃ´t
git clone https://github.com/AsyncFuncAI/deepwiki-open.git
cd deepwiki-open

# CrÃ©er un fichier .env avec vos clÃ©s API
echo "GOOGLE_API_KEY=votre_clÃ©_google" > .env
echo "OPENAI_API_KEY=votre_clÃ©_openai" >> .env
# Facultatif : clÃ© OpenRouter
echo "OPENROUTER_API_KEY=votre_clÃ©_openrouter" >> .env
# Facultatif : hÃ´te personnalisÃ© Ollama
echo "OLLAMA_HOST=votre_hote_ollama" >> .env
# Facultatif : Azure OpenAI
echo "AZURE_OPENAI_API_KEY=votre_clÃ©_azure" >> .env
echo "AZURE_OPENAI_ENDPOINT=votre_endpoint" >> .env
echo "AZURE_OPENAI_VERSION=version_api" >> .env

# Lancer avec Docker Compose
docker-compose up
```

Pour des instructions dÃ©taillÃ©es sur lâ€™utilisation de DeepWiki avec Ollama et Docker, consultez [Ollama Instructions](Ollama-instruction.md).

> ğŸ’¡ **OÃ¹ obtenir ces clÃ©s :**
> - Obtenez une clÃ© API Google depuis [Google AI Studio](https://makersuite.google.com/app/apikey)
> - Obtenez une clÃ© API OpenAI depuis [OpenAI Platform](https://platform.openai.com/api-keys)
> - Obtenez les identifiants Azure OpenAI depuis [Azure Portal](https://portal.azure.com/) â€“ crÃ©ez une ressource Azure OpenAI et rÃ©cupÃ©rez la clÃ© API, lâ€™endpoint et la version de lâ€™API

### Option 2 : Installation manuelle (RecommandÃ©e)

#### Ã‰tape 1 : Configurez vos clÃ©s API

CrÃ©ez un fichier `.env` Ã  la racine du projet avec ces clÃ©s :
```
GOOGLE_API_KEY=votre_clÃ©_google
OPENAI_API_KEY=votre_clÃ©_openai
# Optionnel : Ajoutez ceci pour utiliser des modÃ¨les OpenRouter
OPENROUTER_API_KEY=votre_clÃ©_openrouter
# Optionnel : Ajoutez ceci pour utiliser des modÃ¨les Azure OpenAI
AZURE_OPENAI_API_KEY=votre_clÃ©_azure_openai
AZURE_OPENAI_ENDPOINT=votre_endpoint_azure_openai
AZURE_OPENAI_VERSION=votre_version_azure_openai
# Optionnel :Ajouter un hÃ´te distant Ollama si il n'est pas local. dÃ©faut : http://localhost:11434
OLLAMA_HOST=votre_hote_ollama
```

#### Ã‰tape 2 : DÃ©marrer le Backend

```bash
# Installer dÃ©pendances Python
pip install -r api/requirements.txt

# DÃ©marrer le serveur API
python -m api.main
```

#### Ã‰tape 3 : DÃ©marrer le Frontend

```bash
# Installer les dÃ©pendances JavaScript
npm install
# ou
yarn install

# DÃ©marrer le serveur web
npm run dev
# ou
yarn dev
```

#### Ã‰tape 4 : Utiliser DeepWiki!

1. Ouvrir [http://localhost:3000](http://localhost:3000) dans votre navigateur
2. Entrer l'adresse d'un dÃ©pÃ´t GitHub, GitLab ou Bitbucket (comme `https://github.com/openai/codex`, `https://github.com/microsoft/autogen`, `https://gitlab.com/gitlab-org/gitlab`, or `https://bitbucket.org/redradish/atlassian_app_versions`)
3. Pour les dÃ©pÃ´ts privÃ©s, cliquez sur "+ Ajouter un jeton d'accÃ¨s" et entrez votre jeton dâ€™accÃ¨s personnel GitHub ou GitLab.
4. Cliquez sur "GÃ©nÃ©rer le Wiki" et regardez la magie opÃ©rer !

## ğŸ” Comment Ã§a marche

DeepWiki utilise l'IA pour :

1. Cloner et analyser le dÃ©pÃ´t GitHub, GitLab ou Bitbucket (y compris les dÃ©pÃ´ts privÃ©s avec authentification par jeton d'accÃ¨s)
2. CrÃ©er des embeddings du code pour une rÃ©cupÃ©ration intelligente
3. GÃ©nÃ©rer de la documentation avec une IA sensible au contexte (en utilisant les modÃ¨les Google Gemini, OpenAI, OpenRouter, Azure OpenAI ou Ollama local)
4. CrÃ©er des diagrammes visuels pour expliquer les relations du code
5. Organiser le tout dans un wiki structurÃ©
6. Permettre des questions-rÃ©ponses intelligentes avec le dÃ©pÃ´t grÃ¢ce Ã  la fonctionnalitÃ© Ask
7. Fournir des capacitÃ©s de recherche approfondie avec DeepResearch

```mermaid
graph TD
    A[Utilisateur entre un dÃ©pÃ´t GitHub/GitLab/Bitbucket] --> AA{DÃ©pÃ´t privÃ©?}
    AA -->|Oui| AB[Ajouter un jeton d'accÃ¨s]
    AA -->|Non| B[Cloner le dÃ©pÃ´t]
    AB --> B
    B --> C[Analyser la structure du code]
    C --> D[CrÃ©er des Embeddings]

    D --> M{SÃ©lectionner le modÃ¨le}
    M -->|Google Gemini| E1[GÃ©nÃ©rer avec Gemini]
    M -->|OpenAI| E2[GÃ©nÃ©rer avec OpenAI]
    M -->|OpenRouter| E3[GÃ©nÃ©rer avec OpenRouter]
    M -->|Local Ollama| E4[GÃ©nÃ©rer avec Ollama]
    M -->|Azure| E5[GÃ©nÃ©rer avec Azure]

    E1 --> E[GÃ©nÃ©rer la documentation]
    E2 --> E
    E3 --> E
    E4 --> E
    E5 --> E

    D --> F[CrÃ©er des diagrammes]
    E --> G[Organiser en Wiki]
    F --> G
    G --> H[DeepWiki interactif]

    classDef process stroke-width:2px;
    classDef data stroke-width:2px;
    classDef result stroke-width:2px;
    classDef decision stroke-width:2px;

    class A,D data;
    class AA,M decision;
    class B,C,E,F,G,AB,E1,E2,E3,E4,E5 process;
    class H result;
```

## ğŸ› ï¸ Structure du Projet

```
deepwiki/
â”œâ”€â”€ api/                  # Serveur API Backend
â”‚   â”œâ”€â”€ main.py           # Point d'entrÃ©e de l'API
â”‚   â”œâ”€â”€ api.py            # ImplÃ©mentation FastAPI
â”‚   â”œâ”€â”€ rag.py            # GÃ©nÃ©ration AugmentÃ©e par RÃ©cupÃ©ration (RAG)
â”‚   â”œâ”€â”€ data_pipeline.py  # Utilitaires de traitement des donnÃ©es
â”‚   â””â”€â”€ requirements.txt  # DÃ©pendances Python
â”‚
â”œâ”€â”€ src/                  # Application Frontend Next.js
â”‚   â”œâ”€â”€ app/              # RÃ©pertoire de l'application Next.js
â”‚   â”‚   â””â”€â”€ page.tsx      # Page principale de l'application
â”‚   â””â”€â”€ components/       # Composants React
â”‚       â””â”€â”€ Mermaid.tsx   # Rendu des diagrammes Mermaid
â”‚
â”œâ”€â”€ public/               # Ressources statiques
â”œâ”€â”€ package.json          # DÃ©pendances JavaScript
â””â”€â”€ .env                  # Variables d'environnement (Ã  crÃ©er)
```

## ğŸ¤– SystÃ¨me de sÃ©lection de modÃ¨les

DeepWiki implÃ©mente dÃ©sormais un systÃ¨me de sÃ©lection de modÃ¨les flexible, qui prend en charge plusieurs fournisseurs de LLM :

### Fournisseurs et modÃ¨les pris en charge

- **Google** : Par dÃ©faut `gemini-2.0-flash`, prend Ã©galement en charge `gemini-1.5-flash`, `gemini-1.0-pro`, etc.
- **OpenAI** : Par dÃ©faut `gpt-4o`, prend Ã©galement en charge `o4-mini`, etc.
- **OpenRouter** : AccÃ¨s Ã  plusieurs modÃ¨les via une API unifiÃ©e, notamment Claude, Llama, Mistral, etc.
- **Azure OpenAI** : Par dÃ©faut `gpt-4o`, prend Ã©galement en charge `o4-mini`, etc.
- **Ollama** : Prise en charge des modÃ¨les open source exÃ©cutÃ©s localement, tels que `llama3`.

### Variables d'environnement

Chaque fournisseur requiert les variables d'environnement de clÃ© API correspondantesÂ :

```
# API Keys
GOOGLE_API_KEY=votre_clÃ©_google        # Requis pour les modÃ¨les Google Gemini
OPENAI_API_KEY=votre_clÃ©_openai        # Requis pour les modÃ¨les OpenAI
OPENROUTER_API_KEY=votre_clÃ©_openrouter # Requis pour les modÃ¨les OpenRouter
AZURE_OPENAI_API_KEY=votre_clÃ©_azure_openai  #Requis pour les modÃ¨les Azure OpenAI
AZURE_OPENAI_ENDPOINT=votre_endpoint_azure_openai  #Requis pour les modÃ¨les Azure OpenAI
AZURE_OPENAI_VERSION=votre_version_azure_openai  #Requis pour les modÃ¨les Azure OpenAI

# Configuration d'un endpoint OpenAI API personnalisÃ©
OPENAI_BASE_URL=https://custom-api-endpoint.com/v1  # Optionnel, pour les endpoints API OpenAI personnalisÃ©s

# HÃ´te Ollama personnalisÃ©
OLLAMA_HOST=votre_hÃ´te_ollama # Optionnel, si Ollama n'est pas local. dÃ©faut: http://localhost:11434

# RÃ©pertoire de configuration
DEEPWIKI_CONFIG_DIR=/chemin/vers/dossier/de/configuration  # Optionnel, pour personaliser le rÃ©pertoire de stockage de la configuration
```

### Fichiers de Configuration

DeepWiki utilise des fichiers de configuration JSON pour gÃ©rer diffÃ©rents aspects du systÃ¨me :

1. **`generator.json`** : Configuration des modÃ¨les de gÃ©nÃ©ration de texte
   - DÃ©finit les fournisseurs de modÃ¨les disponibles (Google, OpenAI, OpenRouter, Azure, Ollama)
   - SpÃ©cifie les modÃ¨les par dÃ©faut et disponibles pour chaque fournisseur
   - Contient des paramÃ¨tres spÃ©cifiques aux modÃ¨les tels que la tempÃ©rature et top_p

2. **`embedder.json`** : Configuration des modÃ¨les d'embedding et du traitement de texte
   - DÃ©finit les modÃ¨les d'embedding pour le stockage vectoriel
   - Contient la configuration du retriever pour RAG
   - SpÃ©cifie les paramÃ¨tres du sÃ©parateur de texte pour le chunking de documents

3. **`repo.json`** : Configuration de la gestion des dÃ©pÃ´ts
   - Contient des filtres de fichiers pour exclure certains fichiers et rÃ©pertoires
   - DÃ©finit les limites de taille des dÃ©pÃ´ts et les rÃ¨gles de traitement

Par dÃ©faut, ces fichiers sont situÃ©s dans le rÃ©pertoire `api/config/`. Vous pouvez personnaliser leur emplacement Ã  l'aide de la variable d'environnement `DEEPWIKI_CONFIG_DIR`.

### SÃ©lection de ModÃ¨les PersonnalisÃ©s pour les Fournisseurs de Services

La fonctionnalitÃ© de sÃ©lection de modÃ¨les personnalisÃ©s est spÃ©cialement conÃ§ue pour les fournisseurs de services qui ont besoin de :

- Offrir plusieurs choix de modÃ¨les d'IA aux utilisateurs au sein de leur organisation
- S'adapter rapidement Ã  l'Ã©volution rapide du paysage des LLM sans modifications de code
- Prendre en charge des modÃ¨les spÃ©cialisÃ©s ou affinÃ©s qui ne figurent pas dans la liste prÃ©dÃ©finie

Les fournisseurs de services peuvent implÃ©menter leurs offres de modÃ¨les en sÃ©lectionnant parmi les options prÃ©dÃ©finies ou en entrant des identifiants de modÃ¨les personnalisÃ©s dans l'interface utilisateur.

### Configuration de l'URL de base pour les canaux privÃ©s d'entreprise

La configuration `base_url` du client OpenAI est principalement conÃ§ue pour les utilisateurs d'entreprise disposant de canaux API privÃ©s. Cette fonctionnalitÃ© :

- Permet la connexion Ã  des points de terminaison API privÃ©s ou spÃ©cifiques Ã  l'entreprise.
- Permet aux organisations d'utiliser leurs propres services LLM auto-hÃ©bergÃ©s ou dÃ©ployÃ©s sur mesure.
- Prend en charge l'intÃ©gration avec des services tiers compatibles avec l'API OpenAI.

**BientÃ´t disponible** : Dans les prochaines mises Ã  jour, DeepWiki prendra en charge un mode oÃ¹ les utilisateurs devront fournir leurs propres clÃ©s API dans les requÃªtes. Cela permettra aux entreprises clientes disposant de canaux privÃ©s d'utiliser leurs accords API existants sans partager leurs informations d'identification avec le dÃ©ploiement DeepWiki.

## ğŸ§© Utilisation de modÃ¨les d'embedding compatibles avec OpenAI (par exemple, Alibaba Qwen)

Si vous souhaitez utiliser des modÃ¨les d'embedding compatibles avec l'API OpenAI (comme Alibaba Qwen), suivez ces Ã©tapes :

1. Remplacez le contenu de `api/config/embedder.json` par celui de `api/config/embedder_openai_compatible.json`.
2. Dans votre fichier `.env` Ã  la racine du projet, dÃ©finissez les variables d'environnement appropriÃ©es, par exemple :
   ```
   OPENAI_API_KEY=votre_clÃ©_api
   OPENAI_BASE_URL=votre_endpoint_compatible_openai
   ```
3. Le programme substituera automatiquement les espaces rÃ©servÃ©s dans `embedder.json` avec les valeurs de vos variables d'environnement.

Cela vous permet de passer facilement Ã  n'importe quel service d'embedding compatible avec OpenAI sans modifications de code.

### Journalisation (Logging)

DeepWiki utilise le module `logging` intÃ©grÃ© de Python pour la sortie de diagnostics. Vous pouvez configurer la verbositÃ© et la destination du fichier journal via des variables d'environnement :

| Variable        | Description                                                               | Valeur par dÃ©faut             |
|-----------------|---------------------------------------------------------------------------|------------------------------|
| `LOG_LEVEL`     | Niveau de journalisation (DEBUG, INFO, WARNING, ERROR, CRITICAL).         | INFO                         |
| `LOG_FILE_PATH` | Chemin vers le fichier journal. Si dÃ©fini, les journaux y seront Ã©crits.  | `api/logs/application.log`   |

Pour activer la journalisation de dÃ©bogage et diriger les journaux vers un fichier personnalisÃ© :
```bash
export LOG_LEVEL=DEBUG
export LOG_FILE_PATH=./debug.log
python -m api.main
```
Ou avec Docker Compose:
```bash
LOG_LEVEL=DEBUG LOG_FILE_PATH=./debug.log docker-compose up
```

Lors de l'exÃ©cution avec Docker Compose, le rÃ©pertoire `api/logs` du conteneur est liÃ© Ã  `./api/logs` sur votre hÃ´te (voir la section `volumes` dans `docker-compose.yml`), ce qui garantit que les fichiers journaux persistent lors des redÃ©marrages.

Vous pouvez Ã©galement stocker ces paramÃ¨tres dans votre fichier `.env`Â :

```bash
LOG_LEVEL=DEBUG
LOG_FILE_PATH=./debug.log
```
Puis exÃ©cutez simplementÂ :

```bash
docker-compose up
```

**ConsidÃ©rations de sÃ©curitÃ© concernant le chemin des journaux :** Dans les environnements de production, assurez-vous que le rÃ©pertoire `api/logs` et tout chemin de fichier journal personnalisÃ© sont sÃ©curisÃ©s avec des permissions de systÃ¨me de fichiers et des contrÃ´les d'accÃ¨s appropriÃ©s. L'application s'assure que `LOG_FILE_PATH` se trouve dans le rÃ©pertoire `api/logs` du projet afin d'empÃªcher le parcours de chemin ou les Ã©critures non autorisÃ©es.

## ğŸ› ï¸ Configuration AvancÃ©e

### Variables d'environnement

| Variable                | Description                                                     | Requis     | Note                                                                                                     |
|-------------------------|-----------------------------------------------------------------|------------|----------------------------------------------------------------------------------------------------------|
| `GOOGLE_API_KEY`        | ClÃ© API Google Gemini pour la gÃ©nÃ©ration                        | Non        | Requis uniquement si vous souhaitez utiliser les modÃ¨les Google Gemini                                   |
| `OPENAI_API_KEY`        | ClÃ© API OpenAI pour les embeddings et la gÃ©nÃ©ration              | Oui        | RemarqueÂ : Ceci est requis mÃªme si vous n'utilisez pas les modÃ¨les OpenAI, car elle est utilisÃ©e pour les embeddings. |
| `OPENROUTER_API_KEY`    | ClÃ© API OpenRouter pour les modÃ¨les alternatifs                 | Non        | Requis uniquement si vous souhaitez utiliser les modÃ¨les OpenRouter                                      |
| `AZURE_OPENAI_API_KEY`  | ClÃ© API Azure OpenAI                                            | Non        | Requis uniquement si vous souhaitez utiliser les modÃ¨les Azure OpenAI                                    |
| `AZURE_OPENAI_ENDPOINT` | Point de terminaison Azure OpenAI                               | Non        | Requis uniquement si vous souhaitez utiliser les modÃ¨les Azure OpenAI                                    |
| `AZURE_OPENAI_VERSION`  | Version Azure OpenAI                                            | Non        | Requis uniquement si vous souhaitez utiliser les modÃ¨les Azure OpenAI                                    |
| `OLLAMA_HOST`           | HÃ´te Ollama (par dÃ©fautÂ : http://localhost:11434)               | Non        | Requis uniquement si vous souhaitez utiliser un serveur Ollama externe                                   |
| `PORT`                  | Port du serveur API (par dÃ©fautÂ : 8001)                         | Non        | Si vous hÃ©bergez l'API et le frontend sur la mÃªme machine, assurez-vous de modifier le port de `SERVER_BASE_URL` en consÃ©quence |
| `SERVER_BASE_URL`       | URL de base du serveur API (par dÃ©fautÂ : http://localhost:8001) | Non        |                                                                                                           |
| `DEEPWIKI_AUTH_MODE`    | DÃ©finir sur `true` ou `1` pour activer le mode verrouillÃ©        | Non        | La valeur par dÃ©faut est `false`. Si activÃ©, `DEEPWIKI_AUTH_CODE` est requis.                             |
| `DEEPWIKI_AUTH_CODE`    | Le code requis pour la gÃ©nÃ©ration de wiki lorsque `DEEPWIKI_AUTH_MODE` est activÃ©. | Non        | UtilisÃ© uniquement si `DEEPWIKI_AUTH_MODE` est `true` ou `1`.                          |

Si vous n'utilisez pas le mode Ollama, vous devez configurer une clÃ© API OpenAI pour les embeddings. Les autres clÃ©s API ne sont requises que si vous configurez et utilisez des modÃ¨les des fournisseurs correspondants.

## Mode vÃ©rouillÃ©

DeepWiki peut Ãªtre configurÃ© pour fonctionner en mode vÃ©rouillÃ©, oÃ¹ la gÃ©nÃ©ration de wiki nÃ©cessite un code d'autorisation valide. Ceci est utile si vous souhaitez contrÃ´ler qui peut utiliser la fonctionnalitÃ© de gÃ©nÃ©ration.
Restreint l'initialisation du frontend et protÃ¨ge la suppression du cache, mais n'empÃªche pas complÃ¨tement la gÃ©nÃ©ration backend si les points de terminaison de l'API sont atteints directement.

Pour activer le mode vÃ©rouillÃ©, dÃ©finissez les variables d'environnement suivantesÂ :

- `DEEPWIKI_AUTH_MODE`Â : dÃ©finissez cette variable sur `true` ou `1`. Une fois activÃ©e, l'interface affichera un champ de saisie pour le code d'autorisation.
- `DEEPWIKI_AUTH_CODE`Â : dÃ©finissez cette variable sur le code secret souhaitÃ©. Restreint l'initialisation du frontend et protÃ¨ge la suppression du cache, mais n'empÃªche pas complÃ¨tement la gÃ©nÃ©ration backend si les points de terminaison de l'API sont atteints directement.

Si `DEEPWIKI_AUTH_MODE` n'est pas dÃ©fini ou est dÃ©fini sur `false` (ou toute autre valeur que `true`/`1`), la fonctionnalitÃ© d'autorisation sera dÃ©sactivÃ©e et aucun code ne sera requis.

### Configuration Docker

Vous pouvez utiliser Docker pour exÃ©cuter DeepWikiÂ :

#### ExÃ©cution du conteneur

```bash
# RÃ©cupÃ©rer l'image depuis GitHub Container Registry
docker pull ghcr.io/asyncfuncai/deepwiki-open:latest

# ExÃ©cuter le conteneur avec les variables d'environnement
docker run -p 8001:8001 -p 3000:3000 \
  -e GOOGLE_API_KEY=votre_clÃ©_google \
  -e OPENAI_API_KEY=votre_clÃ©_openai \
  -e OPENROUTER_API_KEY=votre_clÃ©_openrouter \
  -e OLLAMA_HOST=votre_hÃ´te_ollama \
  -e AZURE_OPENAI_API_KEY=votre_clÃ©_azure_openai \
  -e AZURE_OPENAI_ENDPOINT=votre_endpoint_azure_openai \
  -e AZURE_OPENAI_VERSION=votre_version_azure_openai \

  -v ~/.adalflow:/root/.adalflow \
  ghcr.io/asyncfuncai/deepwiki-open:latest
```

Cette commande monte Ã©galement `~/.adalflow` de votre hÃ´te vers `/root/.adalflow` dans le conteneur. Ce chemin est utilisÃ© pour stockerÂ :
- Les dÃ©pÃ´ts clonÃ©s (`~/.adalflow/repos/`)
- Leurs embeddings et index (`~/.adalflow/databases/`)
- Le contenu wiki gÃ©nÃ©rÃ© mis en cache (`~/.adalflow/wikicache/`)

Cela garantit que vos donnÃ©es persistent mÃªme si le conteneur est arrÃªtÃ© ou supprimÃ©.

Vous pouvez Ã©galement utiliser le fichier `docker-compose.yml` fourniÂ :

```bash
# Modifiez d'abord le fichier .env avec vos clÃ©s API
docker-compose up
```

(Le fichier `docker-compose.yml` est prÃ©configurÃ© pour monter `~/.adalflow` pour la persistance des donnÃ©es, de maniÃ¨re similaire Ã  la commande `docker run` ci-dessus.)

#### Utilisation d'un fichier .env avec Docker

Vous pouvez Ã©galement monter un fichier `.env` dans le conteneurÂ :

```bash
# CrÃ©er un fichier .env avec vos clÃ©s API
echo "GOOGLE_API_KEY=votre_clÃ©_google" > .env
echo "OPENAI_API_KEY=votre_clÃ©_openai" >> .env
echo "OPENROUTER_API_KEY=votre_clÃ©_openrouter" >> .env
echo "AZURE_OPENAI_API_KEY=votre_clÃ©_azure_openai" >> .env
echo "AZURE_OPENAI_ENDPOINT=votre_endpoint_azure_openai" >> .env
echo "AZURE_OPENAI_VERSION=votre_version_azure_openai"  >> .env
echo "OLLAMA_HOST=votre_hÃ´te_ollama" >> .env

# Run the container with the .env file mounted
docker run -p 8001:8001 -p 3000:3000 \
  -v $(pwd)/.env:/app/.env \
  -v ~/.adalflow:/root/.adalflow \
  ghcr.io/asyncfuncai/deepwiki-open:latest
```

Cette commande monte Ã©galement `~/.adalflow` de votre hÃ´te vers `/root/.adalflow` dans le conteneur. Ce chemin est utilisÃ© pour stockerÂ :
- Les dÃ©pÃ´ts clonÃ©s (`~/.adalflow/repos/`)
- Leurs embeddings et index (`~/.adalflow/databases/`)
- Le contenu wiki gÃ©nÃ©rÃ© mis en cache (`~/.adalflow/wikicache/`)

Cela garantit que vos donnÃ©es persistent mÃªme si le conteneur est arrÃªtÃ© ou supprimÃ©.

#### Construction de l'image Docker localement

If you want to build the Docker image locally:

```bash
# Clone the repository
git clone https://github.com/AsyncFuncAI/deepwiki-open.git
cd deepwiki-open

# Build the Docker image
docker build -t deepwiki-open .

# Run the container
docker run -p 8001:8001 -p 3000:3000 \
  -e GOOGLE_API_KEY=votre_clÃ©_google \
  -e OPENAI_API_KEY=votre_clÃ©_openai \
  -e OPENROUTER_API_KEY=votre_clÃ©_openrouter \
  -e AZURE_OPENAI_API_KEY=votre_clÃ©_azure_openai \
  -e AZURE_OPENAI_ENDPOINT=votre_endpoint_azure_openai \
  -e AZURE_OPENAI_VERSION=votre_version_azure_openai \
  -e OLLAMA_HOST=votre_hÃ´te_ollama \
  deepwiki-open
```

#### Utilisation de certificats auto-signÃ©s dans Docker

Si vous Ãªtes dans un environnement qui utilise des certificats auto-signÃ©s, vous pouvez les inclure dans la construction de l'image DockerÂ :

1. CrÃ©ez un rÃ©pertoire pour vos certificats (le rÃ©pertoire par dÃ©faut est `certs` Ã  la racine de votre projet)
2. Copiez vos fichiers de certificats `.crt` ou `.pem` dans ce rÃ©pertoire
3. Construisez l'image DockerÂ :

```bash
# Construire avec le rÃ©pertoire de certificats par dÃ©faut (certs)
docker build .

# Ou construire avec un rÃ©pertoire de certificats personnalisÃ©
docker build --build-arg CUSTOM_CERT_DIR=my-custom-certs .
```

### DÃ©tails du serveur API

Le serveur API fournitÂ :
- Clonage et indexation des dÃ©pÃ´ts
- RAG (Retrieval Augmented Generation - GÃ©nÃ©ration augmentÃ©e par rÃ©cupÃ©ration)
- ComplÃ©tion de chat en streaming

Pour plus de dÃ©tails, consultez le [README de lâ€™API](./api/README.md).

## ğŸ”Œ IntÃ©gration OpenRouter

DeepWiki prend dÃ©sormais en charge [OpenRouter](https://openrouter.ai/) en tant que fournisseur de modÃ¨les, vous donnant accÃ¨s Ã  des centaines de modÃ¨les d'IA via une seule APIÂ :

- **Options de modÃ¨les multiples**Â : accÃ©dez aux modÃ¨les d'OpenAI, Anthropic, Google, Meta, Mistral, et plus encore
- **Configuration simple**Â : ajoutez simplement votre clÃ© API OpenRouter et sÃ©lectionnez le modÃ¨le que vous souhaitez utiliser
- **RentabilitÃ©**Â : choisissez des modÃ¨les qui correspondent Ã  votre budget et Ã  vos besoins en termes de performances
- **Commutation facile**Â : basculez entre diffÃ©rents modÃ¨les sans modifier votre code

### Comment utiliser OpenRouter avec DeepWiki

1. **Obtenez une clÃ© API**Â : inscrivez-vous sur [OpenRouter](https://openrouter.ai/) et obtenez votre clÃ© API
2. **Ajouter Ã  l'environnement**Â : ajoutez `OPENROUTER_API_KEY=votre_clÃ©` Ã  votre fichier `.env`
3. **Activer dans l'interface utilisateur**Â : cochez l'option "Utiliser l'API OpenRouter" sur la page d'accueil
4. **SÃ©lectionnez le modÃ¨le**Â : choisissez parmi les modÃ¨les populaires tels que GPT-4o, ClaudeÂ 3.5 Sonnet, GeminiÂ 2.0, et plus encore

OpenRouter est particuliÃ¨rement utile si vous souhaitezÂ :

- Essayer diffÃ©rents modÃ¨les sans vous inscrire Ã  plusieurs services
- AccÃ©der Ã  des modÃ¨les qui pourraient Ãªtre restreints dans votre rÃ©gion
- Comparer les performances entre diffÃ©rents fournisseurs de modÃ¨les
- Optimiser le rapport coÃ»t/performance en fonction de vos besoins

## ğŸ¤– FonctionnalitÃ©s Ask & DeepResearch

### FonctionnalitÃ© Ask

La fonctionnalitÃ© Ask vous permet de discuter avec votre dÃ©pÃ´t en utilisant la gÃ©nÃ©ration augmentÃ©e par rÃ©cupÃ©ration (RAG)Â :

- **RÃ©ponses sensibles au contexte**Â : obtenez des rÃ©ponses prÃ©cises basÃ©es sur le code rÃ©el de votre dÃ©pÃ´t
- **AlimentÃ© par RAG**Â : le systÃ¨me rÃ©cupÃ¨re des extraits de code pertinents pour fournir des rÃ©ponses fondÃ©es
- **Streaming en temps rÃ©el**Â : visualisez les rÃ©ponses au fur et Ã  mesure de leur gÃ©nÃ©ration pour une expÃ©rience plus interactive
- **Historique des conversations**Â : le systÃ¨me conserve le contexte entre les questions pour des interactions plus cohÃ©rentes

### FonctionnalitÃ© DeepResearch

DeepResearch fait passer l'analyse de rÃ©fÃ©rentiel au niveau supÃ©rieur avec un processus de recherche en plusieurs Ã©tapesÂ :

- **EnquÃªte approfondie**Â : explore en profondeur des sujets complexes grÃ¢ce Ã  de multiples itÃ©rations de recherche
- **Processus structurÃ©**Â : suit un plan de recherche clair avec des mises Ã  jour et une conclusion complÃ¨te
- **Continuation automatique**Â : l'IA poursuit automatiquement la recherche jusqu'Ã  ce qu'elle atteigne une conclusion (jusqu'Ã  5Â itÃ©rations)
- **Ã‰tapes de la recherche**Â :
  1. **Plan de recherche**Â : dÃ©crit l'approche et les premiÃ¨res conclusions
  2. **Mises Ã  jour de la recherche**Â : s'appuie sur les itÃ©rations prÃ©cÃ©dentes avec de nouvelles informations
  3. **Conclusion finale**Â : fournit une rÃ©ponse complÃ¨te basÃ©e sur toutes les itÃ©rations

Pour utiliser DeepResearch, activez simplement le commutateur "Deep Research" dans l'interface Ask avant de soumettre votre question.

## ğŸ“± Captures d'Ã©cran

![Interface principale de DeepWiki](screenshots/Interface.png)
*L'interface principale de DeepWiki*

![Prise en charge des dÃ©pÃ´ts privÃ©s](screenshots/privaterepo.png)
*AccÃ©dez aux dÃ©pÃ´ts privÃ©s avec des jetons d'accÃ¨s personnels*

![FonctionnalitÃ© DeepResearch](screenshots/DeepResearch.png)
*DeepResearch effectue des recherches en plusieurs Ã©tapes pour des sujets complexes*

### VidÃ©o de dÃ©monstration

[![VidÃ©o de dÃ©mo DeepWiki](https://img.youtube.com/vi/zGANs8US8B4/0.jpg)](https://youtu.be/zGANs8US8B4)

*Regardez DeepWiki en action !*
## â“ DÃ©pannage

### ProblÃ¨mes de clÃ© API

- **"Variables d'environnement manquantes"**Â : assurez-vous que votre fichier `.env` se trouve Ã  la racine du projet et qu'il contient les clÃ©s API requises.
- **"ClÃ© API non valide"**Â : vÃ©rifiez que vous avez correctement copiÃ© la clÃ© complÃ¨te, sans espaces supplÃ©mentaires.
- **"Erreur d'API OpenRouter"**Â : vÃ©rifiez que votre clÃ© API OpenRouter est valide et qu'elle dispose de crÃ©dits suffisants.
- **"Erreur d'API Azure OpenAI"**Â : vÃ©rifiez que vos informations d'identification Azure OpenAI (clÃ© API, point de terminaison et version) sont correctes et que le service est correctement dÃ©ployÃ©.

### ProblÃ¨mes de connexion

- **"Impossible de se connecter au serveur API"**Â : assurez-vous que le serveur API est en cours d'exÃ©cution sur le portÂ 8001.
- **"Erreur CORS"**Â : l'API est configurÃ©e pour autoriser toutes les origines, mais si vous rencontrez des problÃ¨mes, essayez d'exÃ©cuter le frontend et le backend sur la mÃªme machine.

### ProblÃ¨mes de gÃ©nÃ©ration

- **"Erreur lors de la gÃ©nÃ©ration du wiki"**Â : pour les trÃ¨s grands rÃ©fÃ©rentiels, essayez d'abord un rÃ©fÃ©rentiel plus petit.
- **"Format de rÃ©fÃ©rentiel non valide"**Â : assurez-vous que vous utilisez un format d'URL GitHub, GitLab ou Bitbucket valide.
- **"Impossible de rÃ©cupÃ©rer la structure du rÃ©fÃ©rentiel"**Â : pour les rÃ©fÃ©rentiels privÃ©s, assurez-vous d'avoir saisi un jeton d'accÃ¨s personnel valide avec les autorisations appropriÃ©es.
- **"Erreur de rendu du diagramme"**Â : l'application essaiera automatiquement de corriger les diagrammes cassÃ©s.

### Solutions courantes

1. **RedÃ©marrez les deux serveurs**Â : parfois, un simple redÃ©marrage rÃ©sout la plupart des problÃ¨mes.
2. **VÃ©rifiez les journaux de la console**Â : ouvrez les outils de dÃ©veloppement du navigateur pour voir les erreurs JavaScript.
3. **VÃ©rifiez les journaux de l'API**Â : consultez le terminal oÃ¹ l'API est en cours d'exÃ©cution pour les erreurs Python.

## ğŸ¤ Contribution

Les contributions sont les bienvenuesÂ ! N'hÃ©sitez pas Ã Â :
- Ouvrir des issues pour les bugs ou les demandes de fonctionnalitÃ©s
- Soumettre des pull requests pour amÃ©liorer le code
- Partager vos commentaires et vos idÃ©es

## ğŸ“„ Licence

Projet sous licence MIT â€“ Voir le fichier [LICENSE](LICENSE).

## â­ Historique des stars

[![Historique des stars](https://api.star-history.com/svg?repos=AsyncFuncAI/deepwiki-open&type=Date)](https://star-history.com/#AsyncFuncAI/deepwiki-open&Date)


